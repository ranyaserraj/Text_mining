# ğŸ“š TECHNIQUES DE TEXT MINING UTILISÃ‰ES

## Vue d'Ensemble

Ce projet utilise **5 grandes familles de techniques** de text mining pour analyser les discours politiques.

---

## 1ï¸âƒ£ PRÃ‰TRAITEMENT (NLP - Natural Language Processing)

### **a) Tokenisation**
**Quoi :** DÃ©couper le texte en unitÃ©s individuelles (mots)
```python
"L'emploi est prioritaire" â†’ ["L'", "emploi", "est", "prioritaire"]
```

**Pourquoi :** Pour analyser chaque mot sÃ©parÃ©ment

---

### **b) Normalisation**
**Quoi :** Mettre tout en minuscules
```python
"EMPLOI" â†’ "emploi"
"Emploi" â†’ "emploi"
```

**Pourquoi :** Pour que "Emploi" et "emploi" soient considÃ©rÃ©s comme le mÃªme mot

---

### **c) Nettoyage**
**Quoi :** Supprimer les Ã©lÃ©ments inutiles
- Ponctuation : `. , ! ? ; :`
- Chiffres : `2021, 1500, 67`
- CaractÃ¨res spÃ©ciaux

**Pourquoi :** Ces Ã©lÃ©ments n'apportent pas de sens pour l'analyse thÃ©matique

---

### **d) Suppression des Stopwords**
**Quoi :** Retirer les mots vides qui n'ont pas de signification
```python
Stopwords : le, la, les, un, une, de, du, et, ou, dans, pour...
```

**Exemple :**
```
AVANT : "le parti propose une solution pour l'emploi"
APRÃˆS : "parti propose solution emploi"
```

**Pourquoi :** Se concentrer sur les mots porteurs de sens

---

## 2ï¸âƒ£ ANALYSE THÃ‰MATIQUE (Topic Detection)

### **a) Classification par Mots-ClÃ©s**
**Quoi :** Identifier les thÃ¨mes en cherchant des mots-clÃ©s spÃ©cifiques

**Exemple - ThÃ¨me "Emploi" :**
```python
mots_clÃ©s = ["emploi", "travail", "chÃ´mage", "recrutement", "stage"]
```

**Comment Ã§a marche :**
1. Pour chaque thÃ¨me, on a une liste de mots-clÃ©s
2. On compte combien de fois ces mots apparaissent dans le texte
3. On classe les thÃ¨mes par nombre d'occurrences

**RÃ©sultat :**
```
PAM : Emploi (29 mentions) â†’ ThÃ¨me principal
```

---

### **b) Extraction de FrÃ©quences**
**Quoi :** Compter la frÃ©quence d'apparition de chaque thÃ¨me

**Formule simple :**
```
FrÃ©quence(ThÃ¨me) = Î£ occurrences de tous les mots-clÃ©s du thÃ¨me
```

**Exemple :**
```
ThÃ¨me SantÃ© pour PAM:
- "santÃ©" : 8 fois
- "mÃ©dical" : 3 fois
- "hÃ´pital" : 5 fois
â†’ Total : 16 mentions
```

---

## 3ï¸âƒ£ ANALYSE DE SENTIMENT (Sentiment Analysis)

### **a) Classification Lexicale**
**Quoi :** Classer les mots selon leur connotation

**3 catÃ©gories :**

**Positif :**
```python
["amÃ©liorer", "renforcer", "dÃ©velopper", "garantir", "crÃ©er", "succÃ¨s"]
```

**NÃ©gatif :**
```python
["crise", "Ã©chec", "corruption", "problÃ¨me", "dÃ©ficit", "pauvretÃ©"]
```

**Neutre :**
```python
["analyser", "observer", "constater", "identifier", "Ã©valuer"]
```

---

### **b) Calcul de Score**
**Formule :**
```python
Score = (Positifs - NÃ©gatifs) / (Positifs + NÃ©gatifs + Neutres)
```

**Exemple PAM :**
```
Positifs : 30
NÃ©gatifs : 3
Neutres : 1
Score = (30 - 3) / (30 + 3 + 1) = 27/34 = 0.79
```

**InterprÃ©tation :**
- Score > 0.1 â†’ Ton **Positif** (propositions)
- Score -0.1 Ã  0.1 â†’ Ton **Neutre**
- Score < -0.1 â†’ Ton **NÃ©gatif** (critiques)

---

## 4ï¸âƒ£ ANALYSE DE CO-OCCURRENCE (Co-occurrence Analysis)

### **Quoi :** Identifier quels thÃ¨mes sont mentionnÃ©s ensemble

### **Comment Ã§a marche :**

**Ã‰tape 1 :** DÃ©couper en segments
```
Texte = "...emploi...social...dÃ©veloppement..."
â†’ Segment 1 (50 mots) : emploi + social + Ã©conomie
â†’ Segment 2 (50 mots) : social + santÃ© + Ã©ducation
```

**Ã‰tape 2 :** DÃ©tecter les thÃ¨mes dans chaque segment
```
Segment 1 contient : [Emploi, Social, Ã‰conomie]
```

**Ã‰tape 3 :** Compter les paires
```
(Emploi, Social) â†’ +1
(Emploi, Ã‰conomie) â†’ +1
(Social, Ã‰conomie) â†’ +1
```

**RÃ©sultat :**
```
PAM : Emploi â†” Social = 13 co-occurrences
â†’ Ces deux thÃ¨mes sont souvent mentionnÃ©s ensemble
```

---

### **ParamÃ¨tres utilisÃ©s :**
- **Taille de fenÃªtre :** 50 mots
- **Overlap :** 50% (25 mots se chevauchent entre segments)

**Pourquoi l'overlap ?**
Pour ne pas "couper" des liens entre thÃ¨mes qui sont Ã  cheval sur 2 segments

---

## 5ï¸âƒ£ VISUALISATION DE DONNÃ‰ES (Data Visualization)

### **a) Graphiques Ã  Barres**
**Technique :** Matplotlib Bar Chart
**Usage :** Comparer l'importance de chaque thÃ¨me par parti

---

### **b) Heatmap (Carte de Chaleur)**
**Technique :** Seaborn Heatmap
**Usage :** Visualiser l'intensitÃ© de tous les thÃ¨mes pour tous les partis simultanÃ©ment

**Principe :**
- Couleur claire â†’ Faible mention
- Couleur foncÃ©e â†’ Forte mention

---

### **c) Nuage de Mots (Word Cloud)**
**Technique :** WordCloud Algorithm
**Usage :** Visualiser les mots les plus frÃ©quents

**Principe :**
- Taille du mot âˆ FrÃ©quence d'apparition
- Les mots les plus grands = les plus importants

---

### **d) Graphique Radar/Spider**
**Technique :** Polar Projection Chart
**Usage :** Comparer plusieurs partis sur plusieurs dimensions

**Principe :**
1. Chaque axe = un thÃ¨me
2. Distance du centre = intensitÃ© (0-100)
3. Relier les points â†’ forme gÃ©omÃ©trique
4. Surface de la forme = complÃ©tude thÃ©matique

**Lecture :**
- Grande surface â†’ Discours complet
- Forme ronde â†’ Ã‰quilibre
- Pics marquÃ©s â†’ SpÃ©cialisation

---

## ğŸ”¬ TECHNIQUES STATISTIQUES UTILISÃ‰ES

### **1. Normalisation**
**Formule :**
```python
Valeur_normalisÃ©e = (Valeur / Max) Ã— 100
```

**Pourquoi :** Pour comparer des partis avec des longueurs de discours diffÃ©rentes

**Exemple :**
```
PI : 141 mentions d'Ã‰conomie (max absolu)
PAM : 14 mentions d'Ã‰conomie
NormalisÃ© : PAM = (14/141) Ã— 100 = 10/100
```

---

### **2. Comptage de FrÃ©quences**
**Technique :** Bag of Words (BoW) simplifiÃ©

**Principe :**
- On ignore l'ordre des mots
- On compte juste combien de fois chaque mot apparaÃ®t

**Exemple :**
```
"emploi emploi santÃ© emploi" â†’ {"emploi": 3, "santÃ©": 1}
```

---

### **3. Filtrage par Seuil**
**Technique :** Threshold Filtering

**Principe :** Garder uniquement ce qui est significatif

**CritÃ¨res appliquÃ©s :**
- Mots de longueur > 2 caractÃ¨res
- ThÃ¨mes avec au moins 1 mention
- Top 10 pour les visualisations

---

## ğŸ“Š MÃ‰TRIQUES CALCULÃ‰ES

### **1. DensitÃ© ThÃ©matique**
```
DensitÃ© = Nombre de mentions / Nombre total de mots
```

**Exemple PAM :**
```
Emploi : 29 mentions / 556 mots = 5.2%
```

---

### **2. Ratio Positif/NÃ©gatif**
```
Ratio = Mots_Positifs / Mots_NÃ©gatifs
```

**Exemple PAM :**
```
30 / 3 = 10:1 (trÃ¨s positif)
```

---

### **3. Taux de Couverture ThÃ©matique**
```
Couverture = Nombre de thÃ¨mes traitÃ©s / Total thÃ¨mes possibles
```

**Exemple PI :**
```
13 thÃ¨mes / 14 possibles = 92.9%
```

---

## ğŸ¯ ALGORITHMES UTILISÃ‰S

### **1. Recherche de Motifs (Pattern Matching)**
**Algorithme :** Substring Search
**ComplexitÃ© :** O(nÃ—m) oÃ¹ n=longueur texte, m=longueur mot-clÃ©

---

### **2. FenÃªtre Glissante (Sliding Window)**
**Usage :** Pour l'analyse de co-occurrence
```python
for i in range(0, len(mots), step):
    segment = mots[i:i+window_size]
    analyser(segment)
```

---

### **3. AgrÃ©gation de DonnÃ©es (Data Aggregation)**
**Technique :** GroupBy + Count
```python
themes_par_parti = {
    'PAM': {'Emploi': 29, 'Social': 26},
    'PI': {'Ã‰conomie': 141, 'Social': 79}
}
```

---

## ğŸ“ FORMULES MATHÃ‰MATIQUES CLÃ‰S

### **1. Score de Sentiment**
```
S = (P - N) / (P + N + U)
```
oÃ¹ P=Positif, N=NÃ©gatif, U=Neutre

---

### **2. Normalisation Min-Max**
```
X_norm = (X - X_min) / (X_max - X_min) Ã— 100
```

---

### **3. FrÃ©quence Relative**
```
F_rel = (Occurrences_thÃ¨me / Total_mots) Ã— 1000
```

---

## ğŸ› ï¸ OUTILS ET BIBLIOTHÃˆQUES

### **Python 3.12**
Langage de programmation

### **Pandas**
- Manipulation de donnÃ©es tabulaires
- DataFrames pour organiser les rÃ©sultats

### **NumPy**
- Calculs mathÃ©matiques
- Normalisation des valeurs

### **Matplotlib**
- Graphiques Ã  barres
- Graphiques radar

### **Seaborn**
- Heatmap
- Styling des graphiques

### **WordCloud**
- GÃ©nÃ©ration des nuages de mots

### **Collections (defaultdict, Counter)**
- Comptage efficace
- Stockage de frÃ©quences

### **Itertools (combinations)**
- GÃ©nÃ©ration de paires pour co-occurrence

### **Re (Regular Expressions)**
- Nettoyage du texte
- Recherche de motifs

---

## ğŸ“ CONCEPTS CLÃ‰S DU TEXT MINING

### **1. Corpus**
L'ensemble des textes analysÃ©s (4 discours)

### **2. Document**
Un texte individuel (1 discours par parti)

### **3. Token**
Une unitÃ© de texte (un mot aprÃ¨s tokenisation)

### **4. Vocabulaire**
L'ensemble unique de tous les mots

### **5. TF (Term Frequency)**
FrÃ©quence d'un terme dans un document

### **6. Feature**
Une caractÃ©ristique mesurable (ex: nombre de mots positifs)

---

## ğŸ“ˆ PIPELINE D'ANALYSE

```
Texte brut
    â†“
1. PRÃ‰TRAITEMENT
   - Tokenisation
   - Normalisation
   - Nettoyage
   - Stopwords
    â†“
2. EXTRACTION
   - ThÃ¨mes
   - Sentiments
   - Co-occurrences
    â†“
3. AGRÃ‰GATION
   - Comptages
   - Calculs de scores
   - Normalisation
    â†“
4. VISUALISATION
   - Graphiques
   - Tableaux
   - Rapports
    â†“
RÃ©sultats finaux
```

---

## ğŸ’¡ POURQUOI CES TECHNIQUES ?

### **PrÃ©traitement**
âœ… NÃ©cessaire pour que l'ordinateur "comprenne" le texte
âœ… RÃ©duit le bruit et se concentre sur l'essentiel

### **Analyse ThÃ©matique**
âœ… Identifie automatiquement les prioritÃ©s
âœ… Permet la comparaison objective

### **Analyse de Sentiment**
âœ… Mesure le ton (positif/nÃ©gatif)
âœ… RÃ©vÃ¨le l'approche (critique vs propositif)

### **Co-occurrence**
âœ… RÃ©vÃ¨le les liens conceptuels
âœ… Montre comment les thÃ¨mes s'articulent

### **Visualisation**
âœ… Rend les donnÃ©es comprÃ©hensibles
âœ… Facilite la comparaison

---

## ğŸ¯ AVANTAGES DE CETTE APPROCHE

### âœ… **ObjectivitÃ©**
Pas d'interprÃ©tation subjective, juste des chiffres

### âœ… **ReproductibilitÃ©**
MÃªme analyse = mÃªmes rÃ©sultats

### âœ… **ScalabilitÃ©**
Fonctionne avec 4 textes ou 400

### âœ… **ExhaustivitÃ©**
Analyse tout le texte, pas juste des extraits

### âœ… **RapiditÃ©**
4 secondes vs plusieurs heures manuellement

---

## ğŸ“š LIMITATIONS

### âš ï¸ **Contexte**
Les mots isolÃ©s perdent parfois leur contexte

### âš ï¸ **Nuances**
L'ironie ou le sarcasme sont difficiles Ã  dÃ©tecter

### âš ï¸ **Mots-clÃ©s**
DÃ©pend de la qualitÃ© de la liste de mots-clÃ©s

### âš ï¸ **Langue**
OptimisÃ© pour le franÃ§ais, nÃ©cessite adaptation pour d'autres langues

---

## ğŸš€ AMÃ‰LIORATIONS POSSIBLES

### **1. TF-IDF**
PondÃ©rer les mots par leur raretÃ©
```
TF-IDF = FrÃ©quence Ã— log(N_documents / N_documents_contenant_mot)
```

### **2. N-grams**
Analyser des groupes de mots (bi-grams, tri-grams)
```
"nouveau modÃ¨le" = 1 token au lieu de 2
```

### **3. Word Embeddings**
Utiliser des vecteurs de mots (Word2Vec, GloVe)

### **4. Machine Learning**
EntraÃ®ner des modÃ¨les de classification automatiques

### **5. Named Entity Recognition (NER)**
Extraire automatiquement les noms (personnes, lieux, organisations)

---

## ğŸ“Š RÃ‰SUMÃ‰ DES TECHNIQUES

| Technique | Objectif | ComplexitÃ© |
|-----------|----------|------------|
| Tokenisation | DÃ©couper en mots | Faible |
| Stopwords | Filtrer mots vides | Faible |
| FrÃ©quences | Compter occurrences | Moyenne |
| Sentiment | Mesurer le ton | Moyenne |
| Co-occurrence | Trouver liens | Ã‰levÃ©e |
| Visualisation | PrÃ©senter rÃ©sultats | Moyenne |

---

## ğŸ“ CONCEPTS AVANCÃ‰S APPLIQUÃ‰S

### **Sliding Window avec Overlap**
AmÃ©liore la dÃ©tection de co-occurrence aux frontiÃ¨res

### **Normalisation Multi-Ã©chelle**
Permet la comparaison de textes de longueurs diffÃ©rentes

### **Projection Polaire**
Visualisation radar pour comparaisons multidimensionnelles

### **AgrÃ©gation HiÃ©rarchique**
Organisation des rÃ©sultats par parti â†’ thÃ¨me â†’ sous-thÃ¨me

---

## ğŸ’» COMPLEXITÃ‰ ALGORITHMIQUE

### **PrÃ©traitement**
O(n) oÃ¹ n = nombre de mots

### **Analyse ThÃ©matique**
O(n Ã— m) oÃ¹ m = nombre de mots-clÃ©s par thÃ¨me

### **Co-occurrence**
O(w Ã— tÂ²) oÃ¹ w = nombre de fenÃªtres, t = nombre de thÃ¨mes

### **Visualisation**
O(p Ã— t) oÃ¹ p = nombre de partis

**ComplexitÃ© totale :** O(n Ã— m) dominante

---

## ğŸ¯ EN RÃ‰SUMÃ‰

Ce projet utilise **5 techniques principales** :

1. **PrÃ©traitement NLP** â†’ Nettoyer et prÃ©parer
2. **Analyse ThÃ©matique** â†’ Identifier les sujets
3. **Analyse de Sentiment** â†’ Mesurer le ton
4. **Co-occurrence** â†’ Trouver les liens
5. **Visualisation** â†’ PrÃ©senter les rÃ©sultats

**Toutes ces techniques ensemble** permettent une analyse **complÃ¨te, objective et visuelle** des discours politiques !

---

**Temps d'exÃ©cution total :** ~4 secondes
**Lignes de code :** 774 lignes Python
**RÃ©sultats :** 14 fichiers gÃ©nÃ©rÃ©s

ğŸ‰ **Une analyse qui prendrait des jours manuellement, rÃ©alisÃ©e en quelques secondes !**


"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   PROJET TEXT MINING - DISCOURS POLITIQUES MAROCAINS (Version COLAB)     â•‘
â•‘                                                                           â•‘
â•‘   Ce code est divisÃ© en 11 parties pour faciliter l'exÃ©cution dans       â•‘
â•‘   Google Colab. Chaque partie peut Ãªtre exÃ©cutÃ©e dans une cellule        â•‘
â•‘   sÃ©parÃ©e avec son explication.                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# ============================================================================
# PARTIE 1 : INSTALLATION DES BIBLIOTHÃˆQUES
# ============================================================================
"""
ðŸ“¦ EXPLICATION PARTIE 1 :
Cette partie installe toutes les bibliothÃ¨ques nÃ©cessaires pour le projet.
- spaCy : Pour la lemmatisation (rÃ©duction des mots Ã  leur forme de base)
- wordcloud : Pour crÃ©er des nuages de mots
- openpyxl : Pour exporter en Excel
- Le modÃ¨le franÃ§ais de spaCy pour l'analyse linguistique

â±ï¸ Temps d'exÃ©cution : ~1-2 minutes
ðŸ’¡ Conseil : N'exÃ©cutez cette partie qu'une seule fois au dÃ©but
"""

# Installation des bibliothÃ¨ques
!pip install spacy wordcloud openpyxl -q

# Installation du modÃ¨le franÃ§ais de spaCy
!python -m spacy download fr_core_news_sm -q

print("âœ… Toutes les bibliothÃ¨ques sont installÃ©es !")


# ============================================================================
# PARTIE 2 : IMPORTATION DES MODULES
# ============================================================================
"""
ðŸ“š EXPLICATION PARTIE 2 :
On importe tous les modules Python nÃ©cessaires :
- os, re : Manipulation de fichiers et expressions rÃ©guliÃ¨res
- collections : Comptage efficace (Counter, defaultdict)
- pandas, numpy : Manipulation de donnÃ©es
- matplotlib, seaborn : Visualisations
- wordcloud : Nuages de mots
- spaCy : Lemmatisation avancÃ©e

â±ï¸ Temps d'exÃ©cution : Quelques secondes
"""

import os
import re
from collections import Counter, defaultdict
from itertools import combinations
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import spacy
import warnings
warnings.filterwarnings('ignore')

# Charger le modÃ¨le franÃ§ais de spaCy
try:
    nlp = spacy.load("fr_core_news_sm")
    print("âœ… ModÃ¨le spaCy franÃ§ais chargÃ© avec succÃ¨s")
except OSError:
    print("âŒ Erreur : ModÃ¨le spaCy non trouvÃ©")
    print("   ExÃ©cutez la PARTIE 1 pour l'installer")
    nlp = None

# Configuration de matplotlib
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (14, 8)
sns.set_style("whitegrid")

print("âœ… Tous les modules sont importÃ©s !")


# ============================================================================
# PARTIE 3 : UPLOAD DES FICHIERS TEXTE
# ============================================================================
"""
ðŸ“‚ EXPLICATION PARTIE 3 :
Cette partie permet d'uploader vos 4 fichiers de discours dans Google Colab :
- PAM_Discours.txt
- PI_Discours.txt
- PJD_Discours.txt
- RNI_Discours.txt

ðŸ’¡ AprÃ¨s exÃ©cution, cliquez sur "Choisir les fichiers" et sÃ©lectionnez vos 4 fichiers

â±ï¸ Temps d'exÃ©cution : DÃ©pend de la taille des fichiers
"""

from google.colab import files

print("ðŸ“ Veuillez uploader vos 4 fichiers de discours (.txt)")
print("   Fichiers attendus : PAM_Discours.txt, PI_Discours.txt, PJD_Discours.txt, RNI_Discours.txt")
print()

uploaded = files.upload()

print()
print(f"âœ… {len(uploaded)} fichier(s) uploadÃ©(s) avec succÃ¨s !")
for filename in uploaded.keys():
    print(f"   â€¢ {filename}")


# ============================================================================
# PARTIE 4 : DÃ‰FINITION DE LA CLASSE ET CONFIGURATION
# ============================================================================
"""
ðŸ—ï¸ EXPLICATION PARTIE 4 :
On dÃ©finit la classe principale "AnalyseTextMining" qui contient :
- Les stopwords (mots vides Ã  ignorer : "le", "la", "de"...)
- Les thÃ¨mes avec leurs mots-clÃ©s (Emploi, SantÃ©, Ã‰conomie...)
- Les dictionnaires de sentiment (positif, nÃ©gatif, neutre)

ðŸ“Š 14 thÃ¨mes dÃ©finis : Ã‰ducation, SantÃ©, Emploi, Ã‰conomie, Logement, Justice,
                       Social, Environnement, Gouvernance, Agriculture, 
                       Tourisme, Droits des Femmes, Jeunesse, Infrastructure

â±ï¸ Temps d'exÃ©cution : InstantanÃ©
ðŸ’¡ Cette partie ne fait que dÃ©finir la structure, pas d'analyse encore
"""

class AnalyseTextMining:
    """Classe principale pour l'analyse de text mining des discours politiques"""
    
    def __init__(self, dossier="."):
        self.dossier = dossier
        self.partis = ["PAM", "PI", "PJD", "RNI"]
        self.textes_bruts = {}
        self.textes_nettoyes = {}
        self.themes_par_parti = {}
        self.sentiments_par_parti = {}
        
        # ===== STOPWORDS (mots vides Ã  ignorer) =====
        self.stopwords_fr = set([
            'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'mais',
            'donc', 'car', 'ni', 'ne', 'dans', 'par', 'pour', 'avec', 'sans',
            'sur', 'sous', 'entre', 'vers', 'chez', 'ce', 'ces', 'cet', 'cette',
            'mon', 'ton', 'son', 'notre', 'votre', 'leur', 'mes', 'tes', 'ses',
            'nos', 'vos', 'leurs', 'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils',
            'elles', 'on', 'qui', 'que', 'quoi', 'dont', 'oÃ¹', 'est', 'sont', 'Ãªtre',
            'avoir', 'faire', 'dire', 'aller', 'voir', 'pouvoir', 'vouloir', 'au',
            'aux', 'Ã ', 'plus', 'moins', 'trÃ¨s', 'tout', 'tous', 'toute', 'toutes',
            'mÃªme', 'si', 'ainsi', 'aussi', 'encore', 'dÃ©jÃ ', 'comme', 'aprÃ¨s',
            'avant', 'pendant', 'depuis', 'alors', 'encore', 'enfin', 'puis',
            's', 'd', 'l', 'c', 'j', 'n', 'm', 't', 'qu', 'Ã©tÃ©', 'a', 'ont', 'Ã©tait'
        ])
        
        # ===== THÃˆMES AVEC MOTS-CLÃ‰S =====
        self.themes_keywords = {
            'Ã‰ducation': ['Ã©cole', 'Ã©ducation', 'enseignement', 'Ã©lÃ¨ves', 'formation', 
                         'universitÃ©', 'formation', 'professeur', 'enseignant', 'scolaritÃ©',
                         'scolaire', 'Ã©tudiants', 'apprentissage', 'prÃ©scolaire'],
            'SantÃ©': ['santÃ©', 'mÃ©dical', 'hÃ´pital', 'soins', 'mÃ©decin', 'amo', 
                     'assurance', 'maladie', 'hospitalier', 'ramedistes', 'ramed',
                     'professionnels', 'prestations'],
            'Emploi': ['emploi', 'travail', 'chÃ´mage', 'recrutement', 'jeunes',
                      'stage', 'entreprise', 'salaire', 'prime', 'postes', 'crÃ©ation'],
            'Ã‰conomie': ['Ã©conomie', 'croissance', 'investissement', 'dÃ©veloppement',
                        'pib', 'industriel', 'secteur', 'ressources', 'financier',
                        'industrie', 'production', 'tissu', 'richesse', 'exportation'],
            'Logement': ['logement', 'habitat', 'construction', 'bÃ¢timents',
                        'immobilier', 'mÃ©nages', 'habitations', 'ruine'],
            'Justice': ['justice', 'droit', 'loi', 'Ã©quitÃ©', 'corruption',
                       'transparence', 'lÃ©gal', 'tribunal', 'Ã©quitable', 'hogra',
                       'lÃ©gislatif', 'rÃ©glementation'],
            'Social': ['social', 'pauvretÃ©', 'solidaritÃ©', 'inÃ©galitÃ©s', 'dignitÃ©',
                      'vulnÃ©rables', 'citoyens', 'population', 'sociÃ©tÃ©', 'mÃ©nages',
                      'personnes', 'Ã¢gÃ©es', 'revenu', 'pouvoir', 'achat'],
            'Environnement': ['environnement', 'eau', 'Ã©nergie', 'hydrique', 'climat',
                            'ressources', 'durabilitÃ©', 'renouvelables', 'Ã©nergÃ©tique',
                            'hydrocarbures', 'pollution', 'Ã©cologique'],
            'Gouvernance': ['gouvernance', 'politique', 'institutions', 'administration',
                          'Ã©tat', 'public', 'rÃ©forme', 'dÃ©mocratie', 'rÃ©gionalisation',
                          'gouvernement', 'autoritÃ©', 'concurrence', 'rÃ©gulation'],
            'Agriculture': ['agriculture', 'rural', 'agriculteurs', 'agricoles', 
                          'cultures', 'territorial', 'territoires', 'oasis', 'montagne'],
            'Tourisme': ['tourisme', 'touristiques', 'hÃ´tel', 'visiteurs'],
            'Droits_Femme': ['femme', 'femmes', 'famille', 'genre', 'Ã©galitÃ©'],
            'Jeunesse': ['jeunesse', 'jeunes', 'jeunesses', 'Ã©tudiants'],
            'Infrastructure': ['infrastructure', 'routes', 'transport', 'autoroutes',
                             'ponts', 'Ã©quipements', 'connectivitÃ©']
        }
        
        # ===== DICTIONNAIRES DE SENTIMENT =====
        self.sentiment_positif = [
            'amÃ©liorer', 'renforcer', 'dÃ©velopper', 'garantir', 'soutenir',
            'crÃ©er', 'augmenter', 'rÃ©duire', 'Ã©largir', 'encourager', 'promouvoir',
            'favoriser', 'valoriser', 'moderniser', 'rÃ©ussir', 'succÃ¨s', 'progrÃ¨s',
            'avancÃ©es', 'meilleur', 'optimiser', 'accompagner', 'confiance', 'restaurer',
            'gÃ©nÃ©raliser', 'protÃ©ger', 'bÃ©nÃ©ficier', 'offre', 'nouvelle', 'nouveau',
            'engagement', 'mobilisation', 'reconquÃªte', 'stratÃ©gie', 'vision'
        ]
        
        self.sentiment_negatif = [
            'crise', 'dÃ©ficit', 'problÃ¨me', 'limite', 'difficultÃ©', 'faible',
            'Ã©chec', 'corruption', 'injustice', 'inÃ©galitÃ©s', 'dÃ©fiance', 'pauvretÃ©',
            'chÃ´mage', 'baisse', 'dÃ©gradation', 'dÃ©faut', 'dÃ©fectueux', 'absence',
            'manque', 'inadaptÃ©', 'insuffisant', 'vulnÃ©rabilitÃ©', 'stress', 'fiasco',
            'catastrophique', 'scandales', 'fraude', 'banditisme', 'dÃ©bÃ¢cle', 'dÃ©faite'
        ]
        
        self.sentiment_neutre = [
            'analyser', 'constater', 'observer', 'considÃ©rer', 'examiner',
            'Ã©tudier', 'Ã©valuer', 'mesurer', 'identifier', 'dÃ©finir', 'Ã©tablir',
            'prÃ©senter', 'dÃ©crire', 'expliquer', 'mentionner', 'indiquer'
        ]

print("âœ… Classe AnalyseTextMining dÃ©finie avec succÃ¨s !")
print(f"   â€¢ {len(AnalyseTextMining('.').themes_keywords)} thÃ¨mes configurÃ©s")
print(f"   â€¢ {len(AnalyseTextMining('.').sentiment_positif)} mots positifs")
print(f"   â€¢ {len(AnalyseTextMining('.').sentiment_negatif)} mots nÃ©gatifs")


# ============================================================================
# PARTIE 5 : MÃ‰THODES DE CHARGEMENT ET PRÃ‰TRAITEMENT
# ============================================================================
"""
ðŸ”§ EXPLICATION PARTIE 5 :
On ajoute les mÃ©thodes pour :
1. Charger les fichiers texte
2. PrÃ©traiter les textes (nettoyage + lemmatisation)

La lemmatisation rÃ©duit chaque mot Ã  sa forme de base :
   "dÃ©veloppons" â†’ "dÃ©velopper"
   "politiques" â†’ "politique"
   
Cela regroupe les variantes d'un mÃªme mot et rÃ©duit le volume de ~50%

â±ï¸ Temps d'exÃ©cution : InstantanÃ© (dÃ©finition seulement)
"""

class AnalyseTextMining(AnalyseTextMining):
    
    def charger_textes(self):
        """Charge tous les fichiers texte des partis"""
        print("=" * 80)
        print("ðŸ“‚ Ã‰TAPE 1: CHARGEMENT DES TEXTES")
        print("=" * 80)
        
        for parti in self.partis:
            fichier = f"{parti}_Discours.txt"
            try:
                with open(fichier, 'r', encoding='utf-8') as f:
                    contenu = f.read()
                    self.textes_bruts[parti] = contenu
                    nb_mots = len(contenu.split())
                    print(f"âœ… {parti}: {nb_mots} mots chargÃ©s")
            except FileNotFoundError:
                print(f"âŒ {parti}: Fichier non trouvÃ©")
                self.textes_bruts[parti] = ""
            except Exception as e:
                print(f"âŒ {parti}: Erreur - {e}")
                self.textes_bruts[parti] = ""
        print()

    def pretraiter_textes(self):
        """PrÃ©traitement: nettoyage, tokenisation et lemmatisation"""
        print("=" * 80)
        print("ðŸ”¬ Ã‰TAPE 2: PRÃ‰TRAITEMENT + LEMMATISATION")
        print("=" * 80)
        
        for parti, texte in self.textes_bruts.items():
            if not texte:
                self.textes_nettoyes[parti] = []
                print(f"âš ï¸ {parti}: Texte vide, ignorÃ©")
                continue
            
            # Avec lemmatisation (spaCy)
            if nlp is not None:
                print(f"â³ {parti}: Lemmatisation en cours...")
                
                # Nettoyage basique
                texte_nettoye = re.sub(r'\d+', '', texte)
                texte_nettoye = re.sub(r'\s+', ' ', texte_nettoye).strip()
                
                # Lemmatisation
                doc = nlp(texte_nettoye)
                
                # Extraire les lemmes
                lemmes = []
                for token in doc:
                    if (not token.is_punct and 
                        not token.is_space and 
                        not token.is_stop and 
                        len(token.lemma_) > 2 and
                        token.lemma_.lower() not in self.stopwords_fr):
                        lemmes.append(token.lemma_.lower())
                
                self.textes_nettoyes[parti] = lemmes
                nb_mots_bruts = len(texte.split())
                print(f"âœ… {parti}: {len(lemmes)} lemmes extraits "
                      f"(rÃ©duit de {nb_mots_bruts} mots bruts)")
            
            # Sans lemmatisation (fallback)
            else:
                print(f"âš ï¸ {parti}: Lemmatisation indisponible, mode simple")
                texte = texte.lower()
                texte = re.sub(r'[^\w\s\-Ã©Ã¨ÃªÃ«Ã Ã¢Ã¤Ã´Ã¶Ã¹Ã»Ã¼Ã§Ã¯Ã®]', ' ', texte)
                texte = re.sub(r'\d+', '', texte)
                texte = re.sub(r'\s+', ' ', texte).strip()
                
                mots = texte.split()
                mots_filtres = [
                    mot for mot in mots 
                    if len(mot) > 2 and mot not in self.stopwords_fr
                ]
                
                self.textes_nettoyes[parti] = mots_filtres
                print(f"âœ… {parti}: {len(mots_filtres)} mots aprÃ¨s nettoyage")
        print()

print("âœ… MÃ©thodes de chargement et prÃ©traitement ajoutÃ©es !")


# ============================================================================
# PARTIE 6 : MÃ‰THODES D'ANALYSE THÃ‰MATIQUE ET DE SENTIMENT
# ============================================================================
"""
ðŸ“Š EXPLICATION PARTIE 6 :
On ajoute les mÃ©thodes pour :
1. Analyser les thÃ¨mes (Topic Mining)
   â†’ Compter combien de fois chaque thÃ¨me est mentionnÃ©
   â†’ Algorithme : Pattern Matching (recherche de mots-clÃ©s)

2. Analyser les sentiments (Sentiment Analysis)
   â†’ Compter les mots positifs, nÃ©gatifs, neutres
   â†’ Calculer un score : (positifs - nÃ©gatifs) / total
   â†’ Score > 0.1 = Positif (propositions)
   â†’ Score < -0.1 = NÃ©gatif (critiques)

â±ï¸ Temps d'exÃ©cution : InstantanÃ© (dÃ©finition seulement)
"""

class AnalyseTextMining(AnalyseTextMining):
    
    def analyser_themes(self):
        """Analyse thÃ©matique: identification des sujets par parti"""
        print("=" * 80)
        print("ðŸŽ¯ Ã‰TAPE 3: ANALYSE THÃ‰MATIQUE (Topic Mining)")
        print("=" * 80)
        
        for parti, mots in self.textes_nettoyes.items():
            if not mots:
                self.themes_par_parti[parti] = {}
                continue
            
            texte_complet = ' '.join(mots)
            themes_detectes = {}
            
            # Compter les mots-clÃ©s de chaque thÃ¨me
            for theme, keywords in self.themes_keywords.items():
                count = sum(texte_complet.count(kw) for kw in keywords)
                if count > 0:
                    themes_detectes[theme] = count
            
            self.themes_par_parti[parti] = themes_detectes
            
            print(f"\nðŸ“Œ {parti}:")
            if themes_detectes:
                for theme, count in sorted(themes_detectes.items(), 
                                          key=lambda x: x[1], reverse=True)[:5]:
                    print(f"  â€¢ {theme}: {count} mentions")
            else:
                print("  Aucun thÃ¨me dÃ©tectÃ©")
        print()

    def analyser_sentiments(self):
        """Analyse de sentiment par parti"""
        print("=" * 80)
        print("ðŸ’¬ Ã‰TAPE 4: ANALYSE DE SENTIMENT (Lexicon-Based)")
        print("=" * 80)
        
        for parti, mots in self.textes_nettoyes.items():
            if not mots:
                self.sentiments_par_parti[parti] = {}
                continue
            
            texte = ' '.join(mots)
            
            # Compter les mots de sentiment
            positif_count = sum(texte.count(mot) for mot in self.sentiment_positif)
            negatif_count = sum(texte.count(mot) for mot in self.sentiment_negatif)
            neutre_count = sum(texte.count(mot) for mot in self.sentiment_neutre)
            
            total = positif_count + negatif_count + neutre_count
            
            if total > 0:
                sentiments = {
                    'Positif': positif_count,
                    'NÃ©gatif': negatif_count,
                    'Neutre': neutre_count,
                    'Score': (positif_count - negatif_count) / total
                }
            else:
                sentiments = {
                    'Positif': 0,
                    'NÃ©gatif': 0,
                    'Neutre': 0,
                    'Score': 0
                }
            
            self.sentiments_par_parti[parti] = sentiments
            
            # DÃ©terminer le ton gÃ©nÃ©ral
            if sentiments['Score'] > 0.1:
                ton = "âœ… Positif (propositions/solutions)"
            elif sentiments['Score'] < -0.1:
                ton = "âŒ NÃ©gatif (critiques)"
            else:
                ton = "âš–ï¸ Neutre/Ã‰quilibrÃ©"
            
            print(f"\nðŸ“Œ {parti}:")
            print(f"  Positif: {sentiments['Positif']} | "
                  f"NÃ©gatif: {sentiments['NÃ©gatif']} | "
                  f"Neutre: {sentiments['Neutre']}")
            print(f"  Score global: {sentiments['Score']:.3f}")
            print(f"  Ton gÃ©nÃ©ral: {ton}")
        print()

print("âœ… MÃ©thodes d'analyse thÃ©matique et de sentiment ajoutÃ©es !")


# ============================================================================
# PARTIE 7 : MÃ‰THODE D'ANALYSE DE CO-OCCURRENCE
# ============================================================================
"""
ðŸ”— EXPLICATION PARTIE 7 :
Cette partie analyse les CO-OCCURRENCES de thÃ¨mes :
â†’ Identifier quels thÃ¨mes sont mentionnÃ©s ensemble

Algorithme : Sliding Window (FenÃªtre glissante)
1. DÃ©couper le texte en segments de 50 mots
2. DÃ©tecter quels thÃ¨mes sont dans chaque segment
3. Compter les paires de thÃ¨mes qui apparaissent ensemble

Exemple : Si "Emploi" et "Social" apparaissent dans 20 segments
          â†’ Co-occurrence forte entre ces 2 thÃ¨mes

â±ï¸ Temps d'exÃ©cution : InstantanÃ© (dÃ©finition seulement)
"""

class AnalyseTextMining(AnalyseTextMining):
    
    def analyser_cooccurrence(self):
        """Analyse de co-occurrence des thÃ¨mes (Sliding Window)"""
        print("=" * 80)
        print("ðŸ”— Ã‰TAPE 5: ANALYSE DE CO-OCCURRENCE (Sliding Window)")
        print("=" * 80)
        
        cooccurrences_par_parti = {}
        
        for parti, mots in self.textes_nettoyes.items():
            if not mots:
                continue
            
            texte = ' '.join(mots)
            
            # FenÃªtre de 50 mots avec overlap de 50%
            window_size = 50
            cooccurrences = defaultdict(int)
            
            words = texte.split()
            for i in range(0, len(words), window_size // 2):
                segment = ' '.join(words[i:i+window_size])
                
                # DÃ©tecter thÃ¨mes dans ce segment
                themes_dans_segment = []
                for theme, keywords in self.themes_keywords.items():
                    if any(kw in segment for kw in keywords):
                        themes_dans_segment.append(theme)
                
                # Compter les paires
                if len(themes_dans_segment) >= 2:
                    for theme1, theme2 in combinations(sorted(themes_dans_segment), 2):
                        cooccurrences[(theme1, theme2)] += 1
            
            cooccurrences_par_parti[parti] = cooccurrences
        
        # Afficher et exporter
        cooccurrence_data = []
        for parti, cooccs in cooccurrences_par_parti.items():
            if cooccs:
                top_cooccs = sorted(cooccs.items(), key=lambda x: x[1], reverse=True)[:10]
                print(f"\nðŸ“Œ {parti} - Top 10 co-occurrences:")
                for (theme1, theme2), count in top_cooccs:
                    print(f"  â€¢ {theme1} â†” {theme2}: {count} fois")
                    cooccurrence_data.append({
                        'Parti': parti,
                        'Theme_1': theme1,
                        'Theme_2': theme2,
                        'Frequence': count
                    })
        
        if cooccurrence_data:
            df_coocc = pd.DataFrame(cooccurrence_data)
            df_coocc.to_csv('cooccurrences_themes.csv', index=False, encoding='utf-8-sig')
            df_coocc.to_excel('cooccurrences_themes.xlsx', index=False)
            print("\nâœ… Analyse de co-occurrence sauvegardÃ©e: cooccurrences_themes.csv/.xlsx")
        
        print()
        return cooccurrences_par_parti

print("âœ… MÃ©thode d'analyse de co-occurrence ajoutÃ©e !")


# ============================================================================
# PARTIE 8 : MÃ‰THODE DE CRÃ‰ATION DE TABLEAUX
# ============================================================================
"""
ðŸ“Š EXPLICATION PARTIE 8 :
Cette partie crÃ©e des tableaux de synthÃ¨se :
1. Tableau comparatif par parti (top thÃ¨mes, ton, score)
2. Tableau dÃ©taillÃ© par thÃ¨me (nombre de mentions par parti)

Export en CSV et Excel pour faciliter l'analyse

â±ï¸ Temps d'exÃ©cution : InstantanÃ© (dÃ©finition seulement)
"""

class AnalyseTextMining(AnalyseTextMining):
    
    def creer_tableau_synthese(self):
        """CrÃ©ation des tableaux de synthÃ¨se"""
        print("=" * 80)
        print("ðŸ“‹ Ã‰TAPE 6: CRÃ‰ATION DES TABLEAUX DE SYNTHÃˆSE")
        print("=" * 80)
        
        # Tableau par parti
        data = []
        for parti in self.partis:
            row = {'Parti': parti}
            row['Nb_Textes'] = 1 if self.textes_bruts.get(parti, "") else 0
            
            # Top 3 thÃ¨mes
            themes = self.themes_par_parti.get(parti, {})
            top_themes = sorted(themes.items(), key=lambda x: x[1], reverse=True)[:3]
            row['Top_Themes'] = ', '.join([f"{t[0]}({t[1]})" for t in top_themes])
            
            # Sentiment
            sentiment = self.sentiments_par_parti.get(parti, {})
            if sentiment.get('Score', 0) > 0.1:
                row['Ton'] = "Positif"
            elif sentiment.get('Score', 0) < -0.1:
                row['Ton'] = "NÃ©gatif"
            else:
                row['Ton'] = "Neutre"
            
            row['Score_Sentiment'] = sentiment.get('Score', 0)
            data.append(row)
        
        df = pd.DataFrame(data)
        print("\nðŸ“Š Tableau comparatif par parti:")
        print(df.to_string(index=False))
        
        df.to_csv('synthese_partis.csv', index=False, encoding='utf-8-sig')
        df.to_excel('synthese_partis.xlsx', index=False)
        print("\nâœ… Tableau sauvegardÃ©: synthese_partis.csv/.xlsx")
        
        # Tableau dÃ©taillÃ© par thÃ¨me
        tous_themes = set()
        for themes in self.themes_par_parti.values():
            tous_themes.update(themes.keys())
        
        themes_detail = []
        for theme in sorted(tous_themes):
            row = {'Theme': theme}
            for parti in self.partis:
                count = self.themes_par_parti.get(parti, {}).get(theme, 0)
                row[parti] = count
            themes_detail.append(row)
        
        df_themes = pd.DataFrame(themes_detail)
        df_themes.to_csv('themes_details.csv', index=False, encoding='utf-8-sig')
        df_themes.to_excel('themes_details.xlsx', index=False)
        print("âœ… DÃ©tails des thÃ¨mes sauvegardÃ©s: themes_details.csv/.xlsx")
        
        print()
        return df, df_themes

print("âœ… MÃ©thode de crÃ©ation de tableaux ajoutÃ©e !")


# ============================================================================
# PARTIE 9 : MÃ‰THODES DE VISUALISATION (GRAPHIQUES)
# ============================================================================
"""
ðŸ“Š EXPLICATION PARTIE 9 :
Cette partie crÃ©e toutes les visualisations :
1. Graphiques Ã  barres (thÃ¨mes par parti)
2. Comparaison des sentiments
3. Nuages de mots
4. Heatmap des thÃ¨mes
5. Graphique radar comparatif

Les graphiques sont sauvegardÃ©s en PNG haute rÃ©solution (300 dpi)

â±ï¸ Temps d'exÃ©cution : InstantanÃ© (dÃ©finition seulement)
"""

class AnalyseTextMining(AnalyseTextMining):
    
    def visualiser_resultats(self):
        """GÃ©nÃ©ration de toutes les visualisations"""
        print("=" * 80)
        print("ðŸ“Š Ã‰TAPE 7: GÃ‰NÃ‰RATION DES VISUALISATIONS")
        print("=" * 80)
        
        # 1. Graphiques Ã  barres par parti
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('RÃ©partition des ThÃ¨mes par Parti Politique', 
                     fontsize=16, fontweight='bold')
        
        for idx, parti in enumerate(self.partis):
            ax = axes[idx // 2, idx % 2]
            themes = self.themes_par_parti.get(parti, {})
            
            if themes:
                top_themes = dict(sorted(themes.items(), 
                                       key=lambda x: x[1], 
                                       reverse=True)[:8])
                
                colors = plt.cm.Set3(range(len(top_themes)))
                ax.barh(list(top_themes.keys()), list(top_themes.values()), 
                       color=colors)
                ax.set_xlabel('Nombre de mentions')
                ax.set_title(f'{parti}', fontweight='bold')
                ax.grid(axis='x', alpha=0.3)
            else:
                ax.text(0.5, 0.5, 'Pas de donnÃ©es', 
                       ha='center', va='center', fontsize=14)
                ax.set_title(f'{parti}', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('themes_par_parti.png', dpi=300, bbox_inches='tight')
        print("âœ… Graphique sauvegardÃ©: themes_par_parti.png")
        plt.show()
        
        # 2. Comparaison des sentiments
        fig, ax = plt.subplots(figsize=(12, 6))
        
        partis_valides = [p for p in self.partis if self.sentiments_par_parti.get(p)]
        scores = [self.sentiments_par_parti[p]['Score'] for p in partis_valides]
        colors = ['green' if s > 0.1 else 'red' if s < -0.1 else 'gray' 
                 for s in scores]
        
        ax.bar(partis_valides, scores, color=colors, alpha=0.7, edgecolor='black')
        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax.set_ylabel('Score de Sentiment', fontsize=12)
        ax.set_xlabel('Parti', fontsize=12)
        ax.set_title('Comparaison des Tons/Sentiments par Parti', 
                    fontsize=14, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='green', alpha=0.7, label='Positif (propositions)'),
            Patch(facecolor='gray', alpha=0.7, label='Neutre'),
            Patch(facecolor='red', alpha=0.7, label='NÃ©gatif (critiques)')
        ]
        ax.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig('sentiments_comparaison.png', dpi=300, bbox_inches='tight')
        print("âœ… Graphique sauvegardÃ©: sentiments_comparaison.png")
        plt.show()
        
        # 3. Nuages de mots
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Nuages de Mots par Parti', fontsize=16, fontweight='bold')
        
        for idx, parti in enumerate(self.partis):
            ax = axes[idx // 2, idx % 2]
            mots = self.textes_nettoyes.get(parti, [])
            
            if mots and len(mots) > 10:
                texte = ' '.join(mots)
                wordcloud = WordCloud(width=800, height=400, 
                                     background_color='white',
                                     colormap='viridis',
                                     max_words=50).generate(texte)
                
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.set_title(f'{parti}', fontweight='bold')
                ax.axis('off')
            else:
                ax.text(0.5, 0.5, 'DonnÃ©es insuffisantes', 
                       ha='center', va='center', fontsize=14)
                ax.set_title(f'{parti}', fontweight='bold')
                ax.axis('off')
        
        plt.tight_layout()
        plt.savefig('nuages_mots.png', dpi=300, bbox_inches='tight')
        print("âœ… Graphique sauvegardÃ©: nuages_mots.png")
        plt.show()
        
        # 4. Heatmap
        tous_themes = set()
        for themes in self.themes_par_parti.values():
            tous_themes.update(themes.keys())
        
        matrix_data = []
        for theme in sorted(tous_themes):
            row = [self.themes_par_parti.get(parti, {}).get(theme, 0) 
                   for parti in self.partis]
            matrix_data.append(row)
        
        if matrix_data:
            fig, ax = plt.subplots(figsize=(10, 12))
            im = ax.imshow(matrix_data, cmap='YlOrRd', aspect='auto')
            
            ax.set_xticks(range(len(self.partis)))
            ax.set_yticks(range(len(tous_themes)))
            ax.set_xticklabels(self.partis)
            ax.set_yticklabels(sorted(tous_themes))
            
            for i in range(len(tous_themes)):
                for j in range(len(self.partis)):
                    text = ax.text(j, i, matrix_data[i][j],
                                 ha="center", va="center", color="black", fontsize=9)
            
            ax.set_title('Heatmap: IntensitÃ© des ThÃ¨mes par Parti', 
                        fontsize=14, fontweight='bold', pad=20)
            plt.colorbar(im, ax=ax, label='Nombre de mentions')
            plt.tight_layout()
            plt.savefig('heatmap_themes.png', dpi=300, bbox_inches='tight')
            print("âœ… Graphique sauvegardÃ©: heatmap_themes.png")
            plt.show()
        
        print()
    
    def visualiser_radar(self):
        """Graphique radar comparatif"""
        print("=" * 80)
        print("ðŸ“Š Ã‰TAPE 8: GRAPHIQUE RADAR COMPARATIF")
        print("=" * 80)
        
        tous_themes = set()
        for themes in self.themes_par_parti.values():
            tous_themes.update(themes.keys())
        
        if not tous_themes:
            print("âš ï¸ Pas de thÃ¨mes Ã  visualiser")
            return
        
        theme_totaux = defaultdict(int)
        for parti_themes in self.themes_par_parti.values():
            for theme, count in parti_themes.items():
                theme_totaux[theme] += count
        
        top_themes = sorted(theme_totaux.items(), key=lambda x: x[1], reverse=True)[:10]
        themes_a_afficher = [t[0] for t in top_themes]
        
        print(f"ðŸ“Œ ThÃ¨mes affichÃ©s (top 10): {', '.join(themes_a_afficher)}")
        
        fig = plt.figure(figsize=(12, 12))
        ax = fig.add_subplot(111, projection='polar')
        
        num_themes = len(themes_a_afficher)
        angles = np.linspace(0, 2 * np.pi, num_themes, endpoint=False).tolist()
        angles += angles[:1]
        
        couleurs = {
            'PAM': '#1f77b4',
            'RNI': '#ff7f0e',
            'PJD': '#2ca02c',
            'PI': '#d62728'
        }
        
        for parti in self.partis:
            themes_parti = self.themes_par_parti.get(parti, {})
            if not themes_parti:
                continue
            
            max_val = max(theme_totaux.values())
            valeurs = [themes_parti.get(theme, 0) / max_val * 100 for theme in themes_a_afficher]
            valeurs += valeurs[:1]
            
            ax.plot(angles, valeurs, 'o-', linewidth=2, label=parti, 
                   color=couleurs.get(parti, 'gray'), markersize=6)
            ax.fill(angles, valeurs, alpha=0.15, color=couleurs.get(parti, 'gray'))
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(themes_a_afficher, size=10)
        ax.set_ylim(0, 100)
        ax.grid(True, linestyle='--', alpha=0.7)
        
        plt.title('Comparaison Radar des ThÃ¨mes par Parti\n(Valeurs normalisÃ©es sur 100)', 
                 size=14, fontweight='bold', pad=20)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)
        
        plt.tight_layout()
        plt.savefig('graphique_radar.png', dpi=300, bbox_inches='tight')
        print("âœ… Graphique radar sauvegardÃ©: graphique_radar.png")
        plt.show()
        
        print()

print("âœ… MÃ©thodes de visualisation ajoutÃ©es !")


# ============================================================================
# PARTIE 10 : MÃ‰THODE DE GÃ‰NÃ‰RATION DE RAPPORT
# ============================================================================
"""
ðŸ“ EXPLICATION PARTIE 10 :
Cette partie gÃ©nÃ¨re un rapport textuel dÃ©taillÃ© contenant :
1. RÃ©sumÃ© exÃ©cutif par parti
2. Analyse comparative des prioritÃ©s
3. Analyse des opinions/sentiments
4. Conclusions

Le rapport est sauvegardÃ© en fichier .txt pour consultation

â±ï¸ Temps d'exÃ©cution : InstantanÃ© (dÃ©finition seulement)
"""

class AnalyseTextMining(AnalyseTextMining):
    
    def generer_rapport(self):
        """GÃ©nÃ¨re un rapport textuel dÃ©taillÃ©"""
        print("=" * 80)
        print("ðŸ“ Ã‰TAPE 9: GÃ‰NÃ‰RATION DU RAPPORT DÃ‰TAILLÃ‰")
        print("=" * 80)
        
        rapport = []
        rapport.append("=" * 80)
        rapport.append("RAPPORT D'ANALYSE - TEXT MINING DES DISCOURS POLITIQUES MAROCAINS")
        rapport.append("=" * 80)
        rapport.append("")
        
        rapport.append("1. RÃ‰SUMÃ‰ EXÃ‰CUTIF")
        rapport.append("-" * 80)
        for parti in self.partis:
            nb_mots = len(self.textes_nettoyes.get(parti, []))
            rapport.append(f"\n{parti}:")
            rapport.append(f"  â€¢ Nombre de mots analysÃ©s: {nb_mots}")
            
            themes = self.themes_par_parti.get(parti, {})
            if themes:
                top3 = sorted(themes.items(), key=lambda x: x[1], reverse=True)[:3]
                rapport.append(f"  â€¢ ThÃ¨mes principaux: {', '.join([t[0] for t in top3])}")
            
            sentiment = self.sentiments_par_parti.get(parti, {})
            score = sentiment.get('Score', 0)
            if score > 0.1:
                ton = "Positif - Focus sur les propositions et solutions"
            elif score < -0.1:
                ton = "NÃ©gatif - Focus sur les critiques et problÃ¨mes"
            else:
                ton = "Neutre/Ã‰quilibrÃ©"
            rapport.append(f"  â€¢ Ton gÃ©nÃ©ral: {ton}")
        
        rapport.append("\n" + "=" * 80)
        rapport.append("2. ANALYSE COMPARATIVE DES PRIORITÃ‰S")
        rapport.append("-" * 80)
        
        themes_par_nb_partis = defaultdict(list)
        tous_themes = set()
        for parti, themes in self.themes_par_parti.items():
            tous_themes.update(themes.keys())
        
        for theme in tous_themes:
            partis_concernes = [p for p in self.partis 
                              if theme in self.themes_par_parti.get(p, {})]
            themes_par_nb_partis[len(partis_concernes)].append(
                (theme, partis_concernes)
            )
        
        rapport.append("\nThÃ¨mes traitÃ©s par tous les partis:")
        if 4 in themes_par_nb_partis or 3 in themes_par_nb_partis:
            communs = themes_par_nb_partis.get(4, []) + themes_par_nb_partis.get(3, [])
            for theme, partis in communs:
                rapport.append(f"  â€¢ {theme}: {', '.join(partis)}")
        else:
            rapport.append("  Aucun thÃ¨me commun Ã  tous les partis")
        
        rapport.append("\nThÃ¨mes spÃ©cifiques Ã  certains partis:")
        if 1 in themes_par_nb_partis:
            for theme, partis in themes_par_nb_partis[1][:5]:
                rapport.append(f"  â€¢ {theme}: {partis[0]} uniquement")
        
        rapport.append("\n" + "=" * 80)
        rapport.append("3. ANALYSE DES OPINIONS/SENTIMENTS")
        rapport.append("-" * 80)
        
        for parti in self.partis:
            sentiment = self.sentiments_par_parti.get(parti, {})
            if sentiment:
                rapport.append(f"\n{parti}:")
                rapport.append(f"  Mots positifs: {sentiment['Positif']}")
                rapport.append(f"  Mots nÃ©gatifs: {sentiment['NÃ©gatif']}")
                rapport.append(f"  Mots neutres: {sentiment['Neutre']}")
                rapport.append(f"  Score: {sentiment['Score']:.3f}")
                
                if sentiment['Score'] > 0.1:
                    rapport.append("  â†’ Approche propositionnelle, focus sur les solutions")
                elif sentiment['Score'] < -0.1:
                    rapport.append("  â†’ Approche critique, focus sur les problÃ¨mes")
                else:
                    rapport.append("  â†’ Approche Ã©quilibrÃ©e")
        
        rapport.append("\n" + "=" * 80)
        rapport.append("4. CONCLUSIONS")
        rapport.append("-" * 80)
        rapport.append("\nObservations principales:")
        rapport.append("â€¢ Les partis abordent des thÃ©matiques variÃ©es reflÃ©tant leurs prioritÃ©s")
        rapport.append("â€¢ Les tons varient entre propositions constructives et critiques")
        rapport.append("â€¢ Certains thÃ¨mes sont communs (dÃ©veloppement, social, Ã©conomie)")
        rapport.append("â€¢ D'autres thÃ¨mes sont spÃ©cifiques Ã  chaque parti")
        
        rapport.append("\n" + "=" * 80)
        rapport.append("FIN DU RAPPORT")
        rapport.append("=" * 80)
        
        rapport_texte = '\n'.join(rapport)
        with open('rapport_analyse.txt', 'w', encoding='utf-8') as f:
            f.write(rapport_texte)
        
        print("âœ… Rapport dÃ©taillÃ© sauvegardÃ©: rapport_analyse.txt")
        print()
        
        return rapport_texte

print("âœ… MÃ©thode de gÃ©nÃ©ration de rapport ajoutÃ©e !")


# ============================================================================
# PARTIE 11 : EXÃ‰CUTION DE L'ANALYSE COMPLÃˆTE
# ============================================================================
"""
ðŸš€ EXPLICATION PARTIE 11 :
Cette partie LANCE L'ANALYSE COMPLÃˆTE !

Elle exÃ©cute toutes les Ã©tapes dans l'ordre :
1. Chargement des textes
2. PrÃ©traitement + Lemmatisation
3. Analyse thÃ©matique (Topic Mining)
4. Analyse de sentiment
5. Analyse de co-occurrence
6. CrÃ©ation des tableaux
7. GÃ©nÃ©ration des visualisations
8. GÃ©nÃ©ration du rapport

â±ï¸ Temps d'exÃ©cution : ~30 secondes Ã  2 minutes selon taille des textes

ðŸ’¾ Fichiers gÃ©nÃ©rÃ©s :
   â€¢ 3 fichiers CSV/Excel (tableaux)
   â€¢ 5-6 graphiques PNG
   â€¢ 1 rapport TXT

ðŸŽ¯ C'est la DERNIÃˆRE partie Ã  exÃ©cuter !
"""

# CrÃ©er l'instance et lancer l'analyse
print("\n")
print("=" * 80)
print("ðŸš€ LANCEMENT DE L'ANALYSE TEXT MINING COMPLÃˆTE")
print("=" * 80)
print("\n")

try:
    # CrÃ©er l'analyseur
    analyseur = AnalyseTextMining(dossier=".")
    
    # ExÃ©cuter toutes les Ã©tapes
    analyseur.charger_textes()
    analyseur.pretraiter_textes()
    analyseur.analyser_themes()
    analyseur.analyser_sentiments()
    analyseur.analyser_cooccurrence()
    df_synthese, df_themes = analyseur.creer_tableau_synthese()
    analyseur.visualiser_resultats()
    analyseur.visualiser_radar()
    rapport = analyseur.generer_rapport()
    
    print("\n")
    print("=" * 80)
    print("âœ… ANALYSE TERMINÃ‰E AVEC SUCCÃˆS ! ðŸŽ‰")
    print("=" * 80)
    print("\nðŸ“ Fichiers gÃ©nÃ©rÃ©s :")
    print("   ðŸ“Š TABLEAUX:")
    print("      â€¢ synthese_partis.csv / .xlsx")
    print("      â€¢ themes_details.csv / .xlsx")
    print("      â€¢ cooccurrences_themes.csv / .xlsx")
    print("\n   ðŸ“ˆ GRAPHIQUES:")
    print("      â€¢ themes_par_parti.png")
    print("      â€¢ sentiments_comparaison.png")
    print("      â€¢ nuages_mots.png")
    print("      â€¢ heatmap_themes.png")
    print("      â€¢ graphique_radar.png")
    print("\n   ðŸ“ RAPPORT:")
    print("      â€¢ rapport_analyse.txt")
    print("\n" + "=" * 80)
    print("ðŸ’¡ Vous pouvez tÃ©lÃ©charger tous les fichiers gÃ©nÃ©rÃ©s !")
    print("=" * 80)
    
    # Afficher le tableau de synthÃ¨se
    print("\nðŸ“Š APERÃ‡U DU TABLEAU DE SYNTHÃˆSE:")
    print(df_synthese.to_string(index=False))
    
except Exception as e:
    print(f"\nâŒ ERREUR: {e}")
    import traceback
    traceback.print_exc()

print("\nðŸŽ“ Analyse terminÃ©e !")

